{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ndg/users/bsteel2/.conda/envs/polar-seeds/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.backend._utils import select_backend\n",
    "import ftlangdetect\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    text = gensim.utils.to_unicode(raw_text, 'utf8', errors='ignore')\n",
    "    text = text.lower()\n",
    "    text = gensim.utils.deaccent(text)\n",
    "    text = re.sub('@[^ ]+', '@user', text)\n",
    "    text = re.sub('http[^ ]+', 'http', text)\n",
    "    return text\n",
    "\n",
    "def check_english(text):\n",
    "    try:\n",
    "        result = ftlangdetect.detect(text)\n",
    "        return result['lang'] == 'en'\n",
    "    except Exception as e:\n",
    "        if str(e) == 'No features in text.':\n",
    "            return False\n",
    "        else:\n",
    "            raise Exception('Unknown error')\n",
    "\n",
    "def check_for_repeating_tokens(tokens):\n",
    "    num_tokens = len(tokens)\n",
    "    num_distinct_tokens = len(set(tokens))\n",
    "    return (num_tokens / num_distinct_tokens) > 4\n",
    "\n",
    "\n",
    "\n",
    "def process_comment(comment):\n",
    "    comment_user = comment['user']\n",
    "    if isinstance(comment_user, str):\n",
    "        author_id = comment_user\n",
    "        author_name = comment_user\n",
    "    elif isinstance(comment_user, dict):\n",
    "        if 'unique_id' in comment_user:\n",
    "            author_id = comment_user['uid']\n",
    "            author_name = comment_user['unique_id']\n",
    "        elif 'uniqueId' in comment_user:\n",
    "            author_id = comment_user['id']\n",
    "            author_name = comment_user['uniqueId']\n",
    "        else:\n",
    "            author_name = ''\n",
    "            author_id = comment_user['uid']\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "    comment_text = comment['text']\n",
    "    return (\n",
    "        comment['cid'],\n",
    "        datetime.fromtimestamp(comment['create_time']), \n",
    "        author_name,\n",
    "        author_id, \n",
    "        comment_text,\n",
    "        comment['aweme_id']\n",
    "    )\n",
    "\n",
    "def load_comments_df():\n",
    "    this_dir_path = os.path.dirname(os.path.abspath(__file__))\n",
    "    data_dir_path = os.path.join(this_dir_path, '..', '..', 'data')\n",
    "\n",
    "    comment_dir_path = os.path.join(data_dir_path, 'comments')\n",
    "\n",
    "    comments_data = []\n",
    "    for file_name in tqdm.tqdm(os.listdir(comment_dir_path)):\n",
    "        file_path = os.path.join(comment_dir_path, file_name, 'video_comments.json')\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            comments = json.load(f)\n",
    "\n",
    "        comments_data += [process_comment(comment) for comment in comments]\n",
    "            \n",
    "    comment_df = pd.DataFrame(comments_data, columns=['comment_id', 'createtime', 'author_name', 'author_id', 'text', 'video_id'])\n",
    "\n",
    "    comment_df = comment_df[comment_df['text'].notna()]\n",
    "    comment_df = comment_df[comment_df['text'] != '']\n",
    "    comment_df = comment_df[comment_df['text'] != 'Nan']\n",
    "\n",
    "    comment_df['text_no_newlines'] = comment_df['text'].str.replace(r'\\n',  ' ', regex=True)\n",
    "    regex_whitespace = '^[\\s ï¸Ž]+$' # evil weird whitespace character\n",
    "    comment_df = comment_df[~comment_df['text_no_newlines'].str.fullmatch(regex_whitespace)]\n",
    "\n",
    "    # get only english comments\n",
    "    comment_df['english'] = comment_df['text_no_newlines'].apply(check_english)\n",
    "    english_comments_df = comment_df[comment_df['english']]\n",
    "\n",
    "    # tokenize\n",
    "    english_comments_df['text_processed'] = english_comments_df['text_no_newlines'].apply(preprocess)\n",
    "\n",
    "    english_comments_df = english_comments_df[english_comments_df['text_processed'].notna()]\n",
    "    english_comments_df = english_comments_df[english_comments_df['text_processed'] != '']\n",
    "\n",
    "    # use first 1 mil\n",
    "    return english_comments_df.iloc[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/ndg/users/bsteel2/.cache/torch/sentence_transformers/cardiffnlp_twitter-roberta-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/ndg/users/bsteel2/.cache/torch/sentence_transformers/cardiffnlp_twitter-roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "9331.63s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9332.75s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9333.12s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9333.54s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "9333.94s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "this_dir_path = os.path.abspath('')\n",
    "data_dir_path = os.path.join(this_dir_path, '..', '..', 'data')\n",
    "\n",
    "df_path = os.path.join(data_dir_path, 'cache', 'half_mil_english_comments.csv')\n",
    "if not os.path.exists(df_path):\n",
    "    final_comments_df = load_comments_df()\n",
    "    final_comments_df.to_csv(df_path)\n",
    "\n",
    "final_comments_df = pd.read_csv(df_path)\n",
    "\n",
    "eng_raw_docs = list(final_comments_df['text_no_newlines'].values)\n",
    "docs = list(final_comments_df['text_processed'].values)\n",
    "timestamps = list(final_comments_df['createtime'].values)\n",
    "\n",
    "# Train the model on the corpus.\n",
    "pretrained_model = 'cardiffnlp/twitter-roberta-base'\n",
    "\n",
    "seed_topic_list = [\n",
    "    ['zelensky', 'slava', 'ukraine', 'hero'],\n",
    "    ['china', 'nato', 'biden', 'trump', 'macron', 'boris'],\n",
    "    ['ura', 'uraa', 'uraaa', 'uraaah', 'putin'],\n",
    "    ['hilarious', 'love', 'tiktok', 'haha']\n",
    "]\n",
    "\n",
    "topic_model = BERTopic(seed_topic_list=None, embedding_model=pretrained_model, nr_topics=100)\n",
    "\n",
    "#model_path = os.path.join(data_dir_path, 'cache', 'model')\n",
    "\n",
    "#if not os.path.exists(model_path):\n",
    "# get embeddings so we can cache\n",
    "embeddings_cache_path = os.path.join(data_dir_path, 'cache', 'english_comment_twitter_roberta_embeddings.npy')\n",
    "if os.path.exists(embeddings_cache_path):\n",
    "    with open(embeddings_cache_path, 'rb') as f:\n",
    "        embeddings = np.load(f)\n",
    "else:\n",
    "    topic_model.embedding_model = select_backend(pretrained_model,\n",
    "                                    language=topic_model.language)\n",
    "    embeddings = topic_model._extract_embeddings(docs,\n",
    "                                                method=\"document\",\n",
    "                                                verbose=topic_model.verbose)\n",
    "    with open(embeddings_cache_path, 'wb') as f:\n",
    "            np.save(f, embeddings)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dir_path = os.path.join(data_dir_path, 'outputs')\n",
    "\n",
    "topic_df = topic_model.get_topic_info()\n",
    "topic_df.to_csv(os.path.join(outputs_dir_path, 'topics.csv'))\n",
    "\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "with open(os.path.join(outputs_dir_path, 'cluster_tree.txt'), 'w') as f:\n",
    "    f.write(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time_df = topic_model.topics_over_time(docs, timestamps, nr_bins=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('polar-seeds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9232894fbd6db67ee5d994d4aaa8f228c5cd1c2f262e94d043d7d32a8c0d1f2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
